{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from hyperopt import hp\n",
    "from ray import tune\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from ray.air.config import RunConfig\n",
    "from ogb.linkproppred import LinkPropPredDataset, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ogbl-biokg' # ogbl-biokg, ogbl-wikikg2\n",
    "\n",
    "rank_path = f\"ranks/{dataset_name}\"\n",
    "checkpoint_path = f\"weights/{dataset_name}_rel_weights.npy\"\n",
    "\n",
    "max_concurrent_trials = 8\n",
    "num_samples = 100\n",
    "num_initial_points = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(name=dataset_name)\n",
    "\n",
    "if dataset_name == 'ogbl-biokg':\n",
    "    model_names = ['KGBench', 'ComplexRP', 'TripleRE']\n",
    "elif dataset_name == 'ogbl-wikikg2':\n",
    "    model_names = model_names = ['Text', 'InterHTPlus', 'StarGraph']\n",
    "else:\n",
    "    raise NotImplementedError(f\"Unsupported dataset: {dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading dataset: {dataset_name}\")\n",
    "ranks = {\n",
    "    'valid': [np.load(f\"{rank_path}/{m}_valid_ranks.npy\") for m in model_names],\n",
    "    'test': [np.load(f\"{rank_path}/{m}_test_ranks.npy\") for m in model_names]\n",
    "}\n",
    "\n",
    "n_model = len(ranks['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(sub_ranks):\n",
    "    new_ranks = 502 - sub_ranks\n",
    "\n",
    "    mrr_head = evaluator.eval({'y_pred_pos': new_ranks[:, 0], 'y_pred_neg': new_ranks[:, 1:501]})['mrr_list'].mean()\n",
    "    mrr_tail = evaluator.eval({'y_pred_pos': new_ranks[:, 501], 'y_pred_neg': new_ranks[:, 502:]})['mrr_list'].mean()\n",
    "\n",
    "    return {'mrr': (mrr_head + mrr_tail) / 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LinkPropPredDataset(name=dataset_name)\n",
    "split_edge = dataset.get_edge_split()\n",
    "train_triples, valid_triples, test_triples = split_edge[\"train\"], split_edge[\"valid\"], split_edge[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == 'ogbl-biokg':\n",
    "    test_relation = test_triples['relation']\n",
    "    valid_relation = valid_triples['relation']\n",
    "    num_relation = int(max(train_triples['relation']))+1\n",
    "elif dataset_name == 'ogbl-wikikg2':\n",
    "    origin_num_relation = int(max(train_triples['relation'].max(), valid_triples['relation'].max(), test_triples['relation'].max()))+1\n",
    "    test_relation = np.concatenate((test_triples['relation'], test_triples['relation'] + origin_num_relation), axis=0)\n",
    "    valid_relation = np.concatenate((valid_triples['relation'], valid_triples['relation'] + origin_num_relation), axis=0)\n",
    "    num_relation = int(max(test_relation.max(), valid_relation.max())) + 1\n",
    "\n",
    "print(num_relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_indexes = {\n",
    "    'valid': {},\n",
    "    'test': {}\n",
    "} # relation_id -> np array\n",
    "\n",
    "for relation_id in range(num_relation):\n",
    "    rel_indexes['test'][relation_id] = np.where(test_relation == relation_id)[0]\n",
    "    rel_indexes['valid'][relation_id] = np.where(valid_relation == relation_id)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(config, data):\n",
    "    sub_ranks = data\n",
    "    weights = [config[f\"w_{i}\"] for i in range(len(sub_ranks))]\n",
    "    ranks_avg = np.average(sub_ranks, weights=weights, axis=0)\n",
    "    mrr = eval_model(ranks_avg)\n",
    "\n",
    "    return mrr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_config = {\n",
    "    'w_0': 0.33,\n",
    "    'w_1': 0.33,\n",
    "    'w_2': 0.34,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "    print(\"Load existing models\")\n",
    "    rel_weights = np.load(checkpoint_path)\n",
    "else:\n",
    "    print(\"Searching for ensemble weights\")\n",
    "    rel_weights = np.zeros((num_relation, n_model))\n",
    "\n",
    "    search_space = {f\"w_{i}\": hp.uniform(f\"w_{i}\", 0, 1)  for i in range(n_model)}\n",
    "    hyperopt_search = HyperOptSearch(search_space, metric=\"mrr\", mode=\"max\", n_initial_points=num_initial_points)\n",
    "\n",
    "    for rel_id in tqdm(range(num_relation)):\n",
    "        if len(rel_indexes['valid'][rel_id]) == 0:\n",
    "            # default weights\n",
    "            rel_weights[rel_id] = np.fromiter(default_config.values(), dtype=np.float32)\n",
    "            continue\n",
    "\n",
    "        subranks = [model_rank[rel_indexes['valid'][rel_id]] for model_rank in ranks['valid']]\n",
    "        tuner = tune.Tuner(tune.with_parameters(objective, data=(subranks)), param_space=search_space,\n",
    "                tune_config=tune.TuneConfig(num_samples=num_samples, search_alg=hyperopt_search, max_concurrent_trials=max_concurrent_trials),\n",
    "                run_config=RunConfig(verbose=0))\n",
    "        results = tuner.fit()\n",
    "\n",
    "        best_weights = np.fromiter(results.get_best_result(metric=\"mrr\", mode=\"max\").config.values(), dtype='float32')\n",
    "        rel_weights[rel_id] = best_weights\n",
    "\n",
    "    # np.save(f\"rel_weights.npy\", rel_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating\")\n",
    "rel_res = {\n",
    "    'test': [{'mrr': 0} for _ in range(num_relation)],\n",
    "    'valid': [{'mrr': 0} for _ in range(num_relation)]\n",
    "}\n",
    "\n",
    "for rel_id in tqdm(range(num_relation)):\n",
    "    # test results\n",
    "    if len(rel_indexes['test'][rel_id]) == 0:\n",
    "        continue\n",
    "    sub_ranks = [model_rank[rel_indexes['test'][rel_id]] for model_rank in ranks['test']]\n",
    "    config = {f\"w_{i}\": rel_weights[rel_id][i] for i in range(n_model)}\n",
    "    metrics = objective(config, (sub_ranks))\n",
    "    rel_res['test'][rel_id] = metrics\n",
    "\n",
    "    # valid results\n",
    "    if len(rel_indexes['valid'][rel_id]) == 0:\n",
    "        continue\n",
    "    sub_ranks = [model_rank[rel_indexes['valid'][rel_id]] for model_rank in ranks['valid']]\n",
    "    config = {f\"w_{i}\": rel_weights[rel_id][i] for i in range(n_model)}\n",
    "    metrics = objective(config, (sub_ranks))\n",
    "    rel_res['valid'][rel_id] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mrr = 0\n",
    "valid_mrr = 0\n",
    "\n",
    "for rel_id in range(num_relation):\n",
    "    test_mrr += rel_res['test'][rel_id]['mrr'] * len(rel_indexes['test'][rel_id])\n",
    "    valid_mrr += rel_res['valid'][rel_id]['mrr'] * len(rel_indexes['valid'][rel_id])\n",
    "\n",
    "test_mrr = test_mrr / ranks['test'][0].shape[0]\n",
    "valid_mrr = valid_mrr / ranks['valid'][0].shape[0]\n",
    "\n",
    "print(f\"Test MRR: {test_mrr}\\nValidation MRR: {valid_mrr}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('th1110')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5f194073aaff85b697eed6685e89bdb344177f148991fb6fdbaca70a347ec53"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
